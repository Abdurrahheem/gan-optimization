{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_sliding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQY7xZd6HLde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0dCG_YH1io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradSliding(Optimizer):\n",
        "    \"\"\"Lan's Gradient Sliding algorithm.\"\"\"\n",
        "    def __init__(self, params, beta, gamma, T):\n",
        "        defaults = dict(beta=beta, gamma=gamma, T=T)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Perform Gradient Sliding step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "            for par in group['params']:\n",
        "                if par.grad is None:\n",
        "                    continue\n",
        "                \n",
        "                state = self.state[par]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 1\n",
        "                    state['mode'] = 'main'\n",
        "                    state['x'] = par.clone()\n",
        "                    state['x_bar'] = par.clone()\n",
        "                    state['x_underbar'] = par.clone()\n",
        "                    state['x_tilde'] = par.clone()\n",
        "                \n",
        "                # Part of main loop before PS (prox-sliding) procedure\n",
        "                if state['mode'] == 'main':\n",
        "                    gamma = group['gamma']\n",
        "                    state['x_underbar'] = (1 - gamma) * state['x_bar'] \\\n",
        "                                        + gamma * state['x']\n",
        "                    state['df_x'] = par.grad\n",
        "                    state['mode'] = 'PS'\n",
        "                    # At the beginning of PS procedure, gradient will be\n",
        "                    # calculated at u0 = x\n",
        "                    par = state['x']\n",
        "                \n",
        "                # PS procedure\n",
        "                elif state['mode'] == 'PS':\n",
        "                    if state['step'] % T == 1:\n",
        "                        state['u'] = par.clone()\n",
        "                        state['u_tilde'] = par.clone()\n",
        "                    dh_u = par.grad\n",
        "                    \n",
        "                    beta = group['beta']\n",
        "                    p = group['p']\n",
        "                    theta = group['theta']\n",
        "\n",
        "                    par = (beta*(state['x'] + p*par) - state['df_x'] - dh_u) \\\n",
        "                        / (beta*(p + 1))\n",
        "                    state['u_tilde'] = (1 - theta) * state['u_tilde'] \\\n",
        "                                     + theta * par\n",
        "                    \n",
        "                    # finish PS procedure\n",
        "                    if state['step'] % T == 0:\n",
        "                        state['x'] = par\n",
        "                        state['x_tilde'] = state['u_tilde']\n",
        "                \n",
        "                        # Part of main loop after PS procedure\n",
        "                        gamma = group['gamma']\n",
        "                        state['x_bar'] = (1 - gamma) * state['x_bar'] \\\n",
        "                                       + gamma * state['x_tilde']\n",
        "                        state['mode'] = 'main'\n",
        "                        # At the beginning of main loop, gradient will be\n",
        "                        # calculated at x_underbar\n",
        "                        par = state['x_underbar']\n",
        "                    state['step'] += 1\n",
        "                \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GESBoqC8MvtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# common training loop\n",
        "for batch_n, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    y_pred = model(x_batch)\n",
        "    \n",
        "    loss = criterion(y_pred, y_batch)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krNVK3fkUsaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what if our loss is a sum of two terms?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}