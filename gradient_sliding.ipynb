{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_sliding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQY7xZd6HLde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from math import ceil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0dCG_YH1io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradSliding(Optimizer):\n",
        "    \"\"\"Lan's Gradient Sliding algorithm.\"\"\"\n",
        "    def __init__(self, params, L, M, D_tilde):\n",
        "        defaults = dict(L=L, M=M, D_tilde=D_tilde)\n",
        "        super().__init__(params, defaults)\n",
        "        self.k = 0\n",
        "        self.t = 0\n",
        "        self.mode = 'main'\n",
        "    \n",
        "    def upd_main_parameters(self):\n",
        "        \"\"\"\n",
        "        Update parameters of main loop of gradient sliding.\n",
        "        \n",
        "        Increment k (counter in main loop). Change mode to PS\n",
        "        (prox-sliding procedure). Calculate parameters according to the\n",
        "        formulas in Lan's book:\n",
        "        gamma, T, beta - formula (8.1.42); T - formula (8.1.42).\n",
        "        gamma_next is value of gamma in the next iteration.\n",
        "        \"\"\"\n",
        "        self.k += 1\n",
        "        self.mode = 'PS'\n",
        "\n",
        "        self.gamma = 3 / (self.k + 2)\n",
        "        self.gamma_next = 3 / (self.k + 3)\n",
        "        \n",
        "        L = self.defaults['L']\n",
        "        M = self.defaults['M']\n",
        "        D_tilde = self.defaults['D_tilde']\n",
        "        T = ceil(M**2 * (self.k + 1)**3 / (D_tilde * L**2))\n",
        "        self.T = int(T)\n",
        "        \n",
        "        self.P = 2 / ((self.T + 1) * (self.T + 2))\n",
        "        self.beta = 9 * L * (1 - self.P) / (2 * (self.k + 1))\n",
        "        # print(f\">>> gamma={self.gamma:.2f}, T={self.T:.2f}, P={self.P:.2f}, beta={self.beta:.2f}\")\n",
        "    \n",
        "    def upd_PS_parameters(self):\n",
        "        \"\"\"\n",
        "        Update parameters of PS procedure.\n",
        "        \n",
        "        Increment t (counter in PS procedure). Calculate p and theta\n",
        "        according to formula (8.1.39) in Lan's book. If this is the last\n",
        "        PS iteration, change mode to main and reset counter.\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "        self.p = self.t / 2\n",
        "        self.theta = 2 * (self.t + 1) / (self.t * (self.t + 3))\n",
        "        # print(f\"p={self.p:.2f}, theta={self.theta:.2f}\")\n",
        "        \n",
        "        if self.t % self.T == 0:\n",
        "            self.t = 0\n",
        "            self.mode = 'main'\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Perform Gradient Sliding step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        \n",
        "        # Part of main loop before PS (prox-sliding) procedure.\n",
        "        # In this branch, par is x_underbar in notation of Lan's book.\n",
        "        if self.mode == 'main':\n",
        "            self.upd_main_parameters()\n",
        "            for group in self.param_groups:\n",
        "                for par in group['params']:\n",
        "                    if par.grad is None:\n",
        "                        continue\n",
        "                    \n",
        "                    state = self.state[par]\n",
        "                    # State initialization.\n",
        "                    if len(state) == 0:\n",
        "                        state['x'] = par.clone()\n",
        "                        state['x_bar'] = par.clone()\n",
        "                    \n",
        "                    state['df_x'] = par.grad\n",
        "                    # At the beginning of PS procedure, gradient of h\n",
        "                    # will be calculated at u0 = x.\n",
        "                    par.copy_(state['x'])\n",
        "                \n",
        "        # PS procedure.\n",
        "        # In this branch, par is u in notation of Lan's book.\n",
        "        elif self.mode == 'PS':\n",
        "            self.upd_PS_parameters()\n",
        "            for group in self.param_groups:\n",
        "                for par in group['params']:\n",
        "                    if par.grad is None:\n",
        "                        continue\n",
        "                    \n",
        "                    state = self.state[par]\n",
        "                    if self.t == 1:\n",
        "                        state['u_tilde'] = par.clone()\n",
        "                    \n",
        "                    dh_u = par.grad\n",
        "\n",
        "                    # Formula (1) from our report.\n",
        "                    numerator = self.beta * (state['x'] + self.p * par) \\\n",
        "                              - state['df_x'] - dh_u\n",
        "                    par.copy_(numerator / (self.beta * (1 + self.p)))\n",
        "                    \n",
        "                    state['u_tilde'] = (1 - self.theta) * state['u_tilde'] \\\n",
        "                                     + self.theta * par\n",
        "                    \n",
        "                    if self.t % self.T == 0:\n",
        "                        # Finish PS procedure.\n",
        "                        state['x'] = par\n",
        "                        state['x_tilde'] = state['u_tilde']\n",
        "                \n",
        "                        # Part of main loop after PS procedure.\n",
        "                        state['x_bar'] = (1 - self.gamma) * state['x_bar'] \\\n",
        "                                       + self.gamma * state['x_tilde']\n",
        "                        # Beginning of main loop of new iteration.\n",
        "                        # Now par is again x_underbar.\n",
        "                        x_underbar = (1 - self.gamma_next) * state['x_bar'] \\\n",
        "                                   + self.gamma_next * state['x']\n",
        "                        par.copy_(x_underbar)\n",
        "                \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4JIcJAY4seS",
        "colab_type": "text"
      },
      "source": [
        "### 1. Model\n",
        "Linear layer without bias: $g(x) = Ax$, where $A \\in \\mathbb{R}^{m \\times n}$ is a feature matrix, $x \\in \\mathbb{R}^n$ is a vector of model's parameters.\n",
        "\n",
        "### 2. RMSE\n",
        "\\begin{equation*}\n",
        "    h(x) = \\sqrt{\\frac{1}{m} \\| g(x) - b \\|^2} = \\frac{1}{\\sqrt{m}} \\| Ax - b \\|,\n",
        "\\end{equation*}\n",
        "where $b$ is target vector. Let us prove that $h(x)$ is Lipschitz continuous:\n",
        "\\begin{equation*}\n",
        "    |h(x) - h(y)| = \\frac{1}{\\sqrt{m}} \\bigl\\lvert \\| Ax - b \\| - \\| Ay - b \\| \\bigr\\rvert \\leq \\frac{1}{\\sqrt{m}} \\| (Ax - b) - (Ay - b) \\| = \\frac{1}{\\sqrt{m}} \\| A(x - y) \\| \\leq \\frac{\\| A \\|}{\\sqrt{m}} \\| x - y \\|.\n",
        "\\end{equation*}\n",
        "Lipschitz constant is $M_h = \\frac{\\| A \\|}{\\sqrt{m}}$. There exist the following theorem:\n",
        "If convex function $h$ is Lipschitz continuous with parameter $M_h$, then for $M = 2M_h$ the following condition holds:\n",
        "\\begin{equation*}\n",
        "    h(x) \\leq h(y)+\\left\\langle h^{\\prime}(y), x-y\\right\\rangle+M\\|x-y\\|\\quad \\forall x, y\n",
        "\\end{equation*}\n",
        "In our case, $M = \\frac{2}{\\sqrt{m}} \\| A \\|$.\n",
        "\n",
        "### 3. $L_2$ regularization\n",
        "\\begin{equation*}\n",
        "    f(x) = \\frac{\\lambda}{n} \\| x \\|^2,\n",
        "\\end{equation*}\n",
        "where $\\lambda > 0$ is regularization coefficient. Let us show that $f(x)$ has Lipschitz continuous gradient:\n",
        "\\begin{equation*}\n",
        "    \\nabla f(x) = \\frac{2 \\lambda}{n} x,\\quad \\| \\nabla f(x) - \\nabla f(y) \\| = \\frac{2 \\lambda}{n} \\| x - y \\|.\n",
        "\\end{equation*}\n",
        "Thus, $f$ has Lipschitz continuous gradient with parameter $L = \\frac{2 \\lambda}{n}$.\n",
        "\n",
        "### 4. Optimization problem\n",
        "\\begin{equation*}\n",
        "    \\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + f(x) = \\frac{1}{\\sqrt{m}} \\| Ax - b \\| + \\frac{\\lambda}{n} \\| x \\|^2 \\right\\}\n",
        "\\end{equation*}\n",
        "Let us bound squared norm of solution. For $x$ to be at least as good as $0$, the following condition should hold:\n",
        "\\begin{equation*}\n",
        "    \\frac{1}{\\sqrt{m}} \\| b \\| \\geq \\frac{\\lambda}{n} \\| x \\|^2 \\iff \\| x \\|^2 \\leq \\frac{n}{\\lambda \\sqrt{m}} \\| b \\| =: R^2\n",
        "\\end{equation*}\n",
        "Now we can reformulate the problem as follows:\n",
        "\\begin{equation*}\n",
        "    \\min_{x \\in X} \\left\\{ h(x) + f(x) \\right\\},\\quad X:= \\{ x \\in \\mathbb{R}^n \\bigm| \\| x \\| \\leq R \\}\n",
        "\\end{equation*}\n",
        "\n",
        "### 5. Bregman's distance\n",
        "Euclidean setup: $V(x, y) = \\frac{1}{2} \\| x - y \\|^2$. Let us find such $D_X^2$ that $V(x, y) \\leq D_X^2\\ \\forall x, y \\in X$.\n",
        "\\begin{equation*}\n",
        "    \\frac{1}{2} \\| x - y \\|^2 \\leq \\frac{1}{2} \\| 2 x_{\\text{max}} \\|^2 = 2 R^2 = \\frac{2n}{\\lambda \\sqrt{m}} \\| b \\| =: D_X^2.\n",
        "\\end{equation*}\n",
        "One of the parameters of gradient sliding is $\\tilde{D} := \\frac{81}{16} D_X^2 = \\frac{81 n}{8 \\lambda \\sqrt{m}} \\| b \\|$ (see page 497 of Lan's book, Corollary 8.2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6XVcgCSsKEk",
        "colab_type": "code",
        "outputId": "70703de6-9856-4715-a278-75fcb09b6daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "m = 100\n",
        "n = 10\n",
        "noise_std = 0.01\n",
        "reg_coef = 1.\n",
        "\n",
        "np.random.seed(0)\n",
        "A = np.random.rand(m, n)\n",
        "np.random.seed(0)\n",
        "x_true = np.random.rand(n)\n",
        "np.random.seed(0)\n",
        "b = A @ x_true + noise_std * np.random.rand(m)\n",
        "\n",
        "M = 2 * np.linalg.norm(A, ord=2) / np.sqrt(m)\n",
        "L = 2 * reg_coef / n\n",
        "D_tilde = 81 * n * np.linalg.norm(b, ord=2) / (8 * reg_coef * np.sqrt(m))\n",
        "print(f\"Algorithm parameters:\\nL={L:.2f}, M={M:.2f}, D_tilde={D_tilde:.2f}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Algorithm parameters:\n",
            "L=0.20, M=3.21, D_tilde=317.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JupERQGVp0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = torch.tensor(A, dtype=torch.float)\n",
        "y_train = torch.tensor(b.reshape(-1, 1), dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBQwwYPsCWkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(10, 1, bias=False)\n",
        "opt = GradSliding(model.parameters(), L, M, D_tilde)\n",
        "\n",
        "mse = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NslW0rHOK7Ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "3e531c37-21cf-4be3-bcd6-6c2b6a7056b2"
      },
      "source": [
        "for i in range(1000):\n",
        "\n",
        "    y_pred = model(X_train)\n",
        "    if opt.mode == 'main':\n",
        "        # Regularization term.\n",
        "        reg = 0\n",
        "        for W in model.parameters():\n",
        "            reg = reg + W.norm(2)**2\n",
        "        loss = reg_coef * reg / n\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            sum_loss = torch.sqrt(mse(y_pred, y_train)) + loss\n",
        "        print(f\"k = {opt.k}, loss = {sum_loss:.2f}\")\n",
        "        if i > 0:\n",
        "            print(f\"T = {opt.T}\")\n",
        "    else:\n",
        "        # RMSE term.\n",
        "        loss = torch.sqrt(mse(y_pred, y_train))\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if i == 0:\n",
        "        print(f\"T = {opt.T}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 0, loss = 3.68\n",
            "T = 7\n",
            "k = 1, loss = 1.10\n",
            "T = 7\n",
            "k = 2, loss = 0.47\n",
            "T = 22\n",
            "k = 3, loss = 0.84\n",
            "T = 53\n",
            "k = 4, loss = 0.54\n",
            "T = 102\n",
            "k = 5, loss = 0.62\n",
            "T = 176\n",
            "k = 6, loss = 0.51\n",
            "T = 280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpqlJWhHfRxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}