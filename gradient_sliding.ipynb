{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_sliding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQY7xZd6HLde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from math import ceil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0dCG_YH1io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradSliding(Optimizer):\n",
        "    \"\"\"Lan's Gradient Sliding algorithm.\"\"\"\n",
        "    def __init__(self, params, L, M, D_tilde):\n",
        "        defaults = dict(L=L, M=M, D_tilde=D_tilde)\n",
        "        super().__init__(params, defaults)\n",
        "    \n",
        "    def compute_gamma(self, k):\n",
        "        \"\"\"Compute gamma according to formula (8.1.42) in Lan's book.\"\"\"\n",
        "        return 3 / (k + 2)\n",
        "    \n",
        "    def compute_T(self, k):\n",
        "        \"\"\"Compute T according to formula (8.1.42) in Lan's book.\"\"\"\n",
        "        L = self.defaults['L']\n",
        "        M = self.defaults['M']\n",
        "        D_tilde = self.defaults['D_tilde']\n",
        "        P = ceil(M**2 * (k + 1)**3 / (D_tilde * L**2))\n",
        "        return int(P)\n",
        "    \n",
        "    def compute_P(self, t):\n",
        "        \"\"\"Compute P according to formula (8.1.44) in Lan's book.\"\"\"\n",
        "        return 2 / ((t + 1) * (t + 2))\n",
        "    \n",
        "    def compute_theta(self, t):\n",
        "        \"\"\"Compute theta according to formula (8.1.39) in Lan's book.\"\"\"\n",
        "        return 2 * (t + 1) / (t * (t + 3))\n",
        "    \n",
        "    def compute_p(self, t):\n",
        "        \"\"\"Compute p according to formula (8.1.39) in Lan's book.\"\"\"\n",
        "        return t / 2\n",
        "    \n",
        "    def compute_beta(self, P, k):\n",
        "        \"\"\"Compute beta according to formula (8.1.42) in Lan's book.\"\"\"\n",
        "        L = self.defaults['L']\n",
        "        return 9 * L * (1 - P) / (2 * (k + 1))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Perform Gradient Sliding step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        \n",
        "        for group in self.param_groups:\n",
        "            for par in group['params']:\n",
        "                if par.grad is None:\n",
        "                    continue\n",
        "                \n",
        "                state = self.state[par]\n",
        "\n",
        "                # State initialization.\n",
        "                if len(state) == 0:\n",
        "                    state['k'] = 0\n",
        "                    state['t'] = 0\n",
        "                    state['mode'] = 'main'\n",
        "                    state['x'] = par.clone()\n",
        "                    state['x_bar'] = par.clone()\n",
        "                \n",
        "                # Part of main loop before PS (prox-sliding) procedure.\n",
        "                # In this branch, par is x_underbar in notation\n",
        "                # of Lan's book.\n",
        "                if state['mode'] == 'main':\n",
        "                    state['k'] += 1\n",
        "                    state['gamma'] = self.compute_gamma(state['k'])\n",
        "                    gamma = state['gamma']\n",
        "                    par = (1 - gamma) * state['x_bar'] + gamma * state['x']\n",
        "                    state['df_x'] = par.grad\n",
        "                    state['mode'] = 'PS'\n",
        "                    # At the beginning of PS procedure, gradient of h\n",
        "                    # will be calculated at u0 = x.\n",
        "                    par = state['x']\n",
        "                    \n",
        "                    state['T'] = self.compute_T(state['k'])\n",
        "                    P = self.compute_P(state['T'])\n",
        "                    state['beta'] = self.compute_beta(P, state['k'])\n",
        "                \n",
        "                # PS procedure.\n",
        "                # In this branch, par is u in notation of Lan's book.\n",
        "                elif state['mode'] == 'PS':\n",
        "                    if state['t'] == 0:\n",
        "                        state['u_tilde'] = par.clone()\n",
        "                    state['t'] += 1\n",
        "                    \n",
        "                    dh_u = par.grad\n",
        "\n",
        "                    p = self.compute_p(state['t'])\n",
        "                    beta = state['beta']\n",
        "\n",
        "                    # Formula (1) from our report.\n",
        "                    par = (beta*(state['x'] + p*par) - state['df_x'] - dh_u) \\\n",
        "                        / (beta*(1 + p))\n",
        "                    theta = self.compute_theta(state['t'])\n",
        "                    state['u_tilde'] = (1 - theta) * state['u_tilde'] \\\n",
        "                                     + theta * par\n",
        "                    \n",
        "                    if state['t'] % state['T'] == 0:\n",
        "                        # Finish PS procedure.\n",
        "                        state['t'] = 0\n",
        "                        state['x'] = par\n",
        "                        state['x_tilde'] = state['u_tilde']\n",
        "                \n",
        "                        # Part of main loop after PS procedure.\n",
        "                        gamma = state['gamma']\n",
        "                        state['x_bar'] = (1 - gamma) * state['x_bar'] \\\n",
        "                                       + gamma * state['x_tilde']\n",
        "                        state['mode'] = 'main'\n",
        "                        # At the beginning of main loop, gradient of f\n",
        "                        # will be calculated at x_underbar.\n",
        "                        par = state['x_underbar']\n",
        "                \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6XVcgCSsKEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75f32149-bff6-4cca-b614-72eb6a13ddcb"
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "import numpy as np\n",
        "\n",
        "n_obj = 100\n",
        "n_feat = 10\n",
        "noise_std = 0.01\n",
        "reg_coef = 0.1\n",
        "\n",
        "np.random.seed(0)\n",
        "A = np.random.rand(n_obj, n_feat)\n",
        "np.random.seed(0)\n",
        "x_true = np.random.rand(n_feat)\n",
        "np.random.seed(0)\n",
        "b = A @ x_true + noise_std * np.random.rand(n_obj)\n",
        "\n",
        "L = 2 * reg_coef / n_obj\n",
        "max_norm = np.linalg.norm(b) / np.sqrt(reg_coef)\n",
        "M = 2 * np.linalg.norm(A, ord=2)**2 * diam / n_obj\n",
        "D_tilde = max_norm**2 * 3 / 2\n",
        "print(L, M, D_tilde)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.002 1022.7681214807048 14705.156360171412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JupERQGVp0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = torch.tensor(A, dtype=torch.float)\n",
        "y_train = torch.tensor(b, dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBQwwYPsCWkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(10, 1)\n",
        "opt = GradSliding(model.parameters(), L, M, D_tilde)\n",
        "\n",
        "loss1 = nn.MSELoss()\n",
        "loss2 = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1nfEzULYL1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model(X_train)\n",
        "f = reg_coef * loss1(y_pred, torch.zeros_like(y_pred))\n",
        "opt.zero_grad()\n",
        "f.backward()\n",
        "opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oShrttnXz7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "28b5166a-2882-4442-f8b5-fb7c1bb16d19"
      },
      "source": [
        "opt.state"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(dict, {Parameter containing:\n",
              "             tensor([0.2415], requires_grad=True): {'T': 142270454,\n",
              "              'beta': 0.0045,\n",
              "              'df_x': None,\n",
              "              'gamma': 1.0,\n",
              "              'k': 1,\n",
              "              'mode': 'PS',\n",
              "              't': 0,\n",
              "              'x': tensor([0.2415]),\n",
              "              'x_bar': tensor([0.2415])},\n",
              "             Parameter containing:\n",
              "             tensor([[-0.2276, -0.1556, -0.0670,  0.0280,  0.0685,  0.0835,  0.1928, -0.2806,\n",
              "                      -0.1542,  0.0517]], requires_grad=True): {'T': 142270454,\n",
              "              'beta': 0.0045,\n",
              "              'df_x': None,\n",
              "              'gamma': 1.0,\n",
              "              'k': 1,\n",
              "              'mode': 'PS',\n",
              "              't': 0,\n",
              "              'x': tensor([[-0.2276, -0.1556, -0.0670,  0.0280,  0.0685,  0.0835,  0.1928, -0.2806,\n",
              "                       -0.1542,  0.0517]]),\n",
              "              'x_bar': tensor([[-0.2276, -0.1556, -0.0670,  0.0280,  0.0685,  0.0835,  0.1928, -0.2806,\n",
              "                       -0.1542,  0.0517]])}})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIht-udcDGLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "\n",
        "    # par = opt.param_groups[0]['params'][0]\n",
        "\n",
        "    y_pred = model(X_train)\n",
        "    f = reg_coef * loss1(y_pred, torch.zeros_like(y_pred))\n",
        "    opt.zero_grad()\n",
        "    f.backward()\n",
        "    opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}